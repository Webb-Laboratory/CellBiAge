{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-cell Model Tuning -- after count binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The **MLAging - all-cell** workflow consists of sections:\n",
    "\n",
    "`00 preprocessing.R` Data preprocessing and preparation in Seurat.\n",
    "\n",
    "`111 All-cell Model Tuning - Before Binarization` ML model tunning using *non-binarized* HVGs and hyperparameter selection using `GridSearchCV`.\n",
    "\n",
    "`112 All-cell Model Tuning - After Binarization` ML model tunning using *binarized* HVGs -- **this notebook**:\n",
    "\n",
    "1. [Data Preparation](#1.-prep)\n",
    "2. [Model Tunning](#2.-tunning)\n",
    "    - [Lasso](#3.-l1)\n",
    "    - [Ridge](#4.-l2)\n",
    "    - [ElasticNet](#5.-eln)\n",
    "    \n",
    "    - [Random Forest](#6.-rfc)\n",
    "    - [XGBoost](#7.-xgbc)\n",
    "    \n",
    "    - [Support Vector Machine with rbf kernel](#8.-svc)\n",
    "\n",
    "`121 All-cell Model 10x - Before Binarization` Run the best models for non-binarized* HVGs over 10 random seeds.\n",
    "\n",
    "`122 All-cell Model 10x - After Binarization` Run the best models for *binarized* HVGs over 10 random seeds.\n",
    " \n",
    "`123 All-cell Model 10x Swapped Train-Test` Run the best models for *binarized* HVGs over 10 random seeds. But switched the training and test sets to make sure that the sequencing throughput did not affect model performance.\n",
    "\n",
    "`13 All-cell Model Result Viz` Result visulization.\n",
    "\n",
    "`14 All-cell ELN Interpretation` Result interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The **MLAging - all-cell** workflow consists of four sections:\n",
    "\n",
    "`00 preprocessing.R` Data preprocessing and preparation in Seurat.\n",
    "\n",
    "`111 All-cell Model Tuning - Before Binarization` ML model tunning using *non-binarized* HVGs and hyperparameter selection using `GridSearchCV`.\n",
    "\n",
    "`112 All-cell Model Tuning - After Binarization` ML model tunning using *binarized* HVGs.\n",
    "\n",
    "- [1. Lasso - L1](#1.-l1)\n",
    "- [2. Ridge - L2](#2.-l2)\n",
    "- [3. ElasticNet](#3.-eln)\n",
    "    \n",
    "    \n",
    "- [4. Random Forest](#4.-rfc)\n",
    "- [5. XGBoost](#5.-xgbc)\n",
    "    \n",
    "    \n",
    "- [6. Support Vector Machine with rbf kernel](#6.-svc)\n",
    "\n",
    "`121 All-cell Model 10x - Before Binarization` Run the best models for non-binarized* HVGs over 10 random seeds -- **this notebook**:\n",
    "\n",
    "`122 All-cell Model 10x - After Binarization` Run the best models for *binarized* HVGs over 10 random seeds.\n",
    " \n",
    "`123 All-cell Model 10x Swapped Train-Test` Run the best models for *binarized* HVGs over 10 random seeds. But switched the training and test sets to make sure that the sequencing throughput did not affect model performance.\n",
    "\n",
    "`13 All-cell Model Result Viz` Result visulization.\n",
    "\n",
    "`14 All-cell ELN Interpretation` Result interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data_processing import *\n",
    "from src.grid_search import *\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "data_type = 'float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation <a name=\"1.-prep\"></a>\n",
    "### Load training, testing batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = '../data/test_final_group_info.csv'\n",
    "input_train = '../data/train_final_group_info.csv'\n",
    "\n",
    "cell_type = 'All'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data prepration for All\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y, custom_cv = data_prep(input_test, input_train,\n",
    "                                                        cell_type, binarization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc_scorer = make_scorer(pr_auc_score, greater_is_better=True,\n",
    "                            needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model tunning<a name=\"2.-tunning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Logistic regression -- l1<a name=\"3.-l1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [32:20<1:37:01, 1940.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 21}\n",
      "best CV score: 0.9645329809120251\n",
      "test score: 0.9607151436507586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [1:05:04<1:05:08, 1954.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 19}\n",
      "best CV score: 0.9647309276427187\n",
      "test score: 0.9607902888704182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [1:37:07<32:19, 1940.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 20}\n",
      "best CV score: 0.964419295822672\n",
      "test score: 0.9607495728651902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [2:09:30<00:00, 1942.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 18}\n",
      "best CV score: 0.9645240269585771\n",
      "test score: 0.9608373502661964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=10000000)\n",
    "# 0.01, 0.05, 0.1, 0.5, 1, 5, 8, 10, 20, 50, 100 \n",
    "# 12.5, 15, 17.5, 20, 25, 30, 35, 40\n",
    "param_grid = {'logisticregression__C': [18, 19, 20, 21, 22, 23]}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                                l1, param_grid, i, custom_cv, pr_auc_scorer)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Logistic regression -- l2<a name=\"4.-l2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [10:02<30:07, 602.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.0774263682681127}\n",
      "best CV score: 0.9752339606733502\n",
      "test score: 0.9669253582815398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [19:56<19:54, 597.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.0774263682681127}\n",
      "best CV score: 0.9752318134393316\n",
      "test score: 0.9669285311511667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [30:45<10:20, 620.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.0774263682681127}\n",
      "best CV score: 0.9752322822811742\n",
      "test score: 0.9669277905646708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [41:40<00:00, 625.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.0774263682681127}\n",
      "best CV score: 0.9752321610156784\n",
      "test score: 0.9669268928317597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "l2 = LogisticRegression(penalty='l2', solver='saga', max_iter=10000000)\n",
    "param_grid = {'logisticregression__C': np.logspace(-2, 2, 10)}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                               l2, param_grid, i, custom_cv, pr_auc_scorer)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Logistic regression -- ElasticNet<a name=\"5.-eln\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [1:14:26<3:43:20, 4466.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.046415888336127774, 'logisticregression__l1_ratio': 0.01}\n",
      "best CV score: 0.9755470672779081\n",
      "test score: 0.9671617154646567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 2/4 [2:28:16<2:28:10, 4445.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.046415888336127774, 'logisticregression__l1_ratio': 0.01}\n",
      "best CV score: 0.975547351794952\n",
      "test score: 0.9671610221966493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 3/4 [3:43:55<1:14:48, 4488.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.046415888336127774, 'logisticregression__l1_ratio': 0.01}\n",
      "best CV score: 0.9755480501504383\n",
      "test score: 0.967161551325769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [4:59:07<00:00, 4486.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.046415888336127774, 'logisticregression__l1_ratio': 0.01}\n",
      "best CV score: 0.9755460582716937\n",
      "test score: 0.9671608627239665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eln = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000000)\n",
    "param_grid = {'logisticregression__C': np.logspace(-2, 1, 10),\n",
    "             'logisticregression__l1_ratio': [0.001, 0.01, 0.05, 0.5, 0.65, 0.8]}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                                eln, param_grid, i, custom_cv, pr_auc_scorer)\n",
    "    \n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Random Forest Classifier<a name=\"6.-rfc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "param_grid = {'randomforestclassifier__max_features': [10, 15, 20, 25, 50, None],\n",
    "              'randomforestclassifier__max_depth': [10, 20, 30, 50, 100, None],\n",
    "              'randomforestclassifier__min_samples_split': [2, 5, 10, 20]}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                                rfc, param_grid, i, custom_cv, pr_auc_scorer)\n",
    "\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) XGBoost Classifier<a name=\"#7.-xgbc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = XGBClassifier(use_label_encoder=False)\n",
    "param_grid = {'xgbclassifier__max_depth': [1, 3, 5, 10, 20, 30, 100],\n",
    "              \"xgbclassifier__learning_rate\": [0.03],\n",
    "              #'xgbclassifier__min_child_weight': [1, 3, 5, 7],\n",
    "              #'xgbclassifier__gamma': [0, 0.1, 0.2 , 0.3, 0.4],\n",
    "              'xgbclassifier__colsample_bytree': [0.9],\n",
    "              'xgbclassifier__subsample': [0.66],\n",
    "              'xgbclassifier__eval_metric': ['logloss']}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                                xgbc, param_grid, i, custom_cv, pr_auc_scorer, xgbc=True)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) SVC<a name=\"8.-svc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(probability=True)\n",
    "# 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, \n",
    "param_grid = {'svc__gamma': [1e-3, 1e-2, 1e-1],\n",
    "              'svc__C': np.logspace(-3, 2, 5)}\n",
    "\n",
    "for i in tqdm(range(4)):\n",
    "    grid, test_score = ML_pipeline_GridSearchCV(train_X, train_y, test_X, test_y, \n",
    "                                                svc, param_grid, i, custom_cv, pr_auc_scorer)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:', grid.best_score_)\n",
    "    print('test score:',test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpymc",
   "language": "python",
   "name": "cellpymc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
