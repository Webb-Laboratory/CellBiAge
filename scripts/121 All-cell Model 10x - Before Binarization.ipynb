{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final ML models over ten random seeds - before count binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The **MLAging - all-cell** workflow consists of sections:\n",
    "\n",
    "`00 preprocessing.R` Data preprocessing and preparation in Seurat.\n",
    "\n",
    "`111 All-cell Model Tuning - Before Binarization` ML model tunning using *non-binarized* HVGs and hyperparameter selection using `GridSearchCV`.\n",
    "\n",
    "`112 All-cell Model Tuning - After Binarization` ML model tunning using *binarized* HVGs.\n",
    "\n",
    "- [1. Lasso - L1](#1.-l1)\n",
    "- [2. Ridge - L2](#2.-l2)\n",
    "- [3. ElasticNet](#3.-eln)\n",
    "    \n",
    "    \n",
    "- [4. Random Forest](#4.-rfc)\n",
    "- [5. XGBoost](#5.-xgbc)\n",
    "    \n",
    "    \n",
    "- [6. Support Vector Machine with rbf kernel](#6.-svc)\n",
    "\n",
    "`121 All-cell Model 10x - Before Binarization` Run the best models for non-binarized* HVGs over 10 random seeds -- **this notebook**:\n",
    "\n",
    "`122 All-cell Model 10x - After Binarization` Run the best models for *binarized* HVGs over 10 random seeds.\n",
    " \n",
    "`123 All-cell Model 10x Swapped Train-Test` Run the best models for *binarized* HVGs over 10 random seeds. But switched the training and test sets to make sure that the sequencing throughput did not affect model performance.\n",
    "\n",
    "`13 All-cell Model Result Viz` Result visulization.\n",
    "\n",
    "`14 All-cell ELN Interpretation` Result interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.preprocessing_eln import *\n",
    "from src.data_processing import *\n",
    "from src.grid_search import *\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "from statistics import mean, stdev\n",
    "\n",
    "data_type = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc_scorer = make_scorer(pr_auc_score, greater_is_better=True,\n",
    "                            needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data prepration for All\n"
     ]
    }
   ],
   "source": [
    "input_test = '../data/test_final_group_info.csv'\n",
    "input_train = '../data/train_final_group_info.csv'\n",
    "\n",
    "cell_type = 'All'\n",
    "\n",
    "train_X, train_y, test_X, test_y, custom_cv = data_prep(input_test, input_train,\n",
    "                                                        cell_type, binarization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. L1 <a name=\"1.-l1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.6485430254049704 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "final_test = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = shuffle(test_X, test_y, random_state=random_state)\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    l1 = LogisticRegression(penalty='l1', C=0.001, solver='saga', max_iter=10000000)\n",
    "        \n",
    "    l1.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = l1.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_test.append((X_test, y_test))\n",
    "    final_models.append(l1)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/l1_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/l1_model_test_sets.save', 'wb')\n",
    "pickle.dump(final_test, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/l1_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. L2 <a name=\"2.-l2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:01<00:00,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.6969592921059651 ± 4.49712617231811e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = final_test[i]\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    l2 = LogisticRegression(penalty='l2', C=0.001, solver='saga', max_iter=10000000)\n",
    "        \n",
    "    l2.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = l2.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_models.append(l2)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/l2_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/l2_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ELN <a name=\"3.-eln\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:59<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.6668618454629941 ± 1.0990021070831686e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = final_test[i]\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    eln = LogisticRegression(penalty='elasticnet', C=0.001, l1_ratio=0.35, \n",
    "                             solver='saga', max_iter=10000000)\n",
    "        \n",
    "    eln.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = eln.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_models.append(eln)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/eln_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/eln_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. RFC <a name=\"4.-rfc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:24<00:00, 38.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.5584284745886156 ± 0.01608135842985142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "scores = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = final_test[i]\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    rfc = RandomForestClassifier(max_features=50, max_depth=10, min_samples_split=5)\n",
    "        \n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rfc.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_models.append(rfc)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/rfc_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/rfc_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. XGBC <a name=\"5.-xgbc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:16<00:00, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.5834939906596455 ± 0.013657010740244533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "scores = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = final_test[i]\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    xgbc = XGBClassifier(max_depth=3, learning_rate=0.03, \n",
    "                     colsample_bytree=0.9, subsample=0.66,\n",
    "                     eval_metric='logloss', use_label_encoder=False)\n",
    "        \n",
    "    xgbc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgbc.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_models.append(xgbc)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/xgbc_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/xgbc_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SVC <a name=\"6.-svc\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [21:38:02<00:00, 7788.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auprc: 0.8286780851998243 ± 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "scores = []\n",
    "final_models = []\n",
    "for i in tqdm(range(10)):\n",
    "    random_state = 42*i    \n",
    "    X_test, y_test = final_test[i]\n",
    "    X_train, y_train = shuffle(train_X, train_y, random_state=random_state)\n",
    "    \n",
    "    svc = SVC(gamma=0.1, C=10, probability=True)\n",
    "        \n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = svc.predict_proba(X_test)[:, 1]\n",
    "    auprc = pr_auc_score(y_test, y_pred)\n",
    "    \n",
    "    final_models.append(svc)\n",
    "    scores.append(auprc)   \n",
    "print(f'auprc: {mean(scores)} ± {stdev(scores)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../results/results_nonbin_best/svc_model_test_scores.save', 'wb')\n",
    "pickle.dump(scores, file)\n",
    "file.close()\n",
    "\n",
    "file = open('../results/results_nonbin_best/svc_model_test_models.save', 'wb')\n",
    "pickle.dump(final_models, file)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellpymc",
   "language": "python",
   "name": "cellpymc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
